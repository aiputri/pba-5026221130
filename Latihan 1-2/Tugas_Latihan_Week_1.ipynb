{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latihan 1: Tokenisasi Teks Indonesia dengan Korpus IndoNLU"
      ],
      "metadata": {
        "id": "QPQH1EHEElNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset IndoNLU SMSA (sentiment analysis)\n",
        "!wget -P ../data/raw https://raw.githubusercontent.com/IndoNLP/indonlu/refs/heads/master/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv\n",
        "!wget -P ../data/raw https://raw.githubusercontent.com/IndoNLP/indonlu/refs/heads/master/dataset/smsa_doc-sentiment-prosa/test_preprocess.tsv\n",
        "!wget -P ../data/raw https://raw.githubusercontent.com/IndoNLP/indonlu/refs/heads/master/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L93CuSx9Ra7x",
        "outputId": "1178ac5b-50cd-48fb-91af-9a2b67f6b93d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-09 02:21:17--  https://raw.githubusercontent.com/IndoNLP/indonlu/refs/heads/master/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2186718 (2.1M) [text/plain]\n",
            "Saving to: ‘../data/raw/train_preprocess.tsv.1’\n",
            "\n",
            "\rtrain_preprocess.ts   0%[                    ]       0  --.-KB/s               \rtrain_preprocess.ts 100%[===================>]   2.08M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-09-09 02:21:17 (28.2 MB/s) - ‘../data/raw/train_preprocess.tsv.1’ saved [2186718/2186718]\n",
            "\n",
            "--2025-09-09 02:21:17--  https://raw.githubusercontent.com/IndoNLP/indonlu/refs/heads/master/dataset/smsa_doc-sentiment-prosa/test_preprocess.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75949 (74K) [text/plain]\n",
            "Saving to: ‘../data/raw/test_preprocess.tsv.1’\n",
            "\n",
            "test_preprocess.tsv 100%[===================>]  74.17K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-09 02:21:17 (3.13 MB/s) - ‘../data/raw/test_preprocess.tsv.1’ saved [75949/75949]\n",
            "\n",
            "--2025-09-09 02:21:17--  https://raw.githubusercontent.com/IndoNLP/indonlu/refs/heads/master/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 246974 (241K) [text/plain]\n",
            "Saving to: ‘../data/raw/valid_preprocess.tsv.1’\n",
            "\n",
            "valid_preprocess.ts 100%[===================>] 241.19K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-09 02:21:17 (6.31 MB/s) - ‘../data/raw/valid_preprocess.tsv.1’ saved [246974/246974]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library yang diperlukan\n",
        "!pip install --upgrade --no-cache-dir nltk PySastrawi\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import Counter\n",
        "\n",
        "# Download tokenizer punkt\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k17u3YejSIr3",
        "outputId": "a3343ed0-0f0d-4587-e0c8-47721d80008d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: PySastrawi in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "p8MaGw-m_sCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff21dea-6067-43a9-87c5-e743fa223524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data train: 11000\n",
            "Jumlah data valid: 1260\n",
            "Jumlah data test : 500\n",
            "\n",
            "Kolom dataset: ['text', 'label']\n",
            "                                                text     label\n",
            "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
            "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
            "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
            "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
            "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
            "\n",
            "Contoh Teks 1 (Label: positive):\n",
            "Teks Asli: warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\n",
            "Token: ['warung', 'ini', 'dimiliki', 'oleh', 'pengusaha', 'pabrik', 'tahu', 'yang', 'sudah', 'puluhan', 'tahun', 'terkenal', 'membuat', 'tahu', 'putih', 'di', 'bandung', '.', 'tahu', 'berkualitas', ',', 'dipadu', 'keahlian', 'memasak', ',', 'dipadu', 'kretivitas', ',', 'jadilah', 'warung', 'yang', 'menyajikan', 'menu', 'utama', 'berbahan', 'tahu', ',', 'ditambah', 'menu', 'umum', 'lain', 'seperti', 'ayam', '.', 'semuanya', 'selera', 'indonesia', '.', 'harga', 'cukup', 'terjangkau', '.', 'jangan', 'lewatkan', 'tahu', 'bletoka', 'nya', ',', 'tidak', 'kalah', 'dengan', 'yang', 'asli', 'dari', 'tegal', '!']\n",
            "Jumlah Token: 66\n",
            "Token Unik: 50\n",
            "Token setelah Stemming: ['warung', 'ini', 'milik', 'oleh', 'usaha', 'pabrik', 'tahu', 'yang', 'sudah', 'puluh', 'tahun', 'kenal', 'buat', 'tahu', 'putih', 'di', 'bandung', 'tahu', 'kualitas', 'padu', 'ahli', 'masak', 'padu', 'kretivitas', 'jadi', 'warung', 'yang', 'saji', 'menu', 'utama', 'bahan', 'tahu', 'tambah', 'menu', 'umum', 'lain', 'seperti', 'ayam', 'semua', 'selera', 'indonesia', 'harga', 'cukup', 'jangkau', 'jangan', 'lewat', 'tahu', 'bletoka', 'nya', 'tidak', 'kalah', 'dengan', 'yang', 'asli', 'dari', 'tegal']\n",
            "Jumlah Token setelah Stemming: 56\n",
            "Token disimpan ke tokens_sample_1.txt\n",
            "\n",
            "Contoh Teks 2 (Label: neutral):\n",
            "Teks Asli: mohon ulama lurus dan k212 mmbri hujjah partai apa yang harus diwlh agar suara islam tidak pecah-pecah\n",
            "Token: ['mohon', 'ulama', 'lurus', 'dan', 'k212', 'mmbri', 'hujjah', 'partai', 'apa', 'yang', 'harus', 'diwlh', 'agar', 'suara', 'islam', 'tidak', 'pecah-pecah']\n",
            "Jumlah Token: 17\n",
            "Token Unik: 17\n",
            "Token setelah Stemming: ['mohon', 'ulama', 'lurus', 'dan', 'k212', 'mmbri', 'hujjah', 'partai', 'apa', 'yang', 'harus', 'diwlh', 'agar', 'suara', 'islam', 'tidak', 'pecah']\n",
            "Jumlah Token setelah Stemming: 17\n",
            "Token disimpan ke tokens_sample_2.txt\n",
            "\n",
            "Contoh Teks 3 (Label: positive):\n",
            "Teks Asli: lokasi strategis di jalan sumatera bandung . tempat nya nyaman terutama sofa di lantai 2 . paella nya enak , sangat pas dimakan dengan minum bir dingin . appetiser nya juga enak-enak .\n",
            "Token: ['lokasi', 'strategis', 'di', 'jalan', 'sumatera', 'bandung', '.', 'tempat', 'nya', 'nyaman', 'terutama', 'sofa', 'di', 'lantai', '2', '.', 'paella', 'nya', 'enak', ',', 'sangat', 'pas', 'dimakan', 'dengan', 'minum', 'bir', 'dingin', '.', 'appetiser', 'nya', 'juga', 'enak-enak', '.']\n",
            "Jumlah Token: 33\n",
            "Token Unik: 27\n",
            "Token setelah Stemming: ['lokasi', 'strategis', 'di', 'jalan', 'sumatera', 'bandung', 'tempat', 'nya', 'nyaman', 'utama', 'sofa', 'di', 'lantai', '2', 'paella', 'nya', 'enak', 'sangat', 'pas', 'makan', 'dengan', 'minum', 'bir', 'dingin', 'appetiser', 'nya', 'juga', 'enak']\n",
            "Jumlah Token setelah Stemming: 28\n",
            "Token disimpan ke tokens_sample_3.txt\n",
            "\n",
            "10 Kata Teratas: [('.', 8), (',', 6), ('tahu', 5), ('yang', 4), ('nya', 4), ('di', 3), ('warung', 2), ('bandung', 2), ('dipadu', 2), ('menu', 2)]\n"
          ]
        }
      ],
      "source": [
        "# Load dataset dengan pandas\n",
        "train_df = pd.read_csv(\"../data/raw/train_preprocess.tsv\", sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
        "valid_df = pd.read_csv(\"../data/raw/valid_preprocess.tsv\", sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
        "test_df  = pd.read_csv(\"../data/raw/test_preprocess.tsv\",  sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
        "\n",
        "print(\"Jumlah data train:\", len(train_df))\n",
        "print(\"Jumlah data valid:\", len(valid_df))\n",
        "print(\"Jumlah data test :\", len(test_df))\n",
        "\n",
        "# Cek kolom dataset\n",
        "print(\"\\nKolom dataset:\", train_df.columns.tolist())\n",
        "print(train_df.head())\n",
        "\n",
        "# Ganti 'sentence' dengan kolom yang benar, biasanya 'text'\n",
        "sample_texts = train_df['text'][:3].tolist()\n",
        "labels = train_df['label'][:3].tolist()\n",
        "\n",
        "# Inisialisasi stemmer\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "# Tokenisasi & analisis\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nContoh Teks {i+1} (Label: {labels[i]}):\")\n",
        "    print(\"Teks Asli:\", text)\n",
        "\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"Token:\", tokens)\n",
        "    print(\"Jumlah Token:\", len(tokens))\n",
        "    print(\"Token Unik:\", len(set(tokens)))\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_text = stemmer.stem(text)\n",
        "    stemmed_tokens = word_tokenize(stemmed_text)\n",
        "    print(\"Token setelah Stemming:\", stemmed_tokens)\n",
        "    print(\"Jumlah Token setelah Stemming:\", len(stemmed_tokens))\n",
        "\n",
        "    # Simpan token ke file\n",
        "    with open(f'tokens_sample_{i+1}.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(tokens))\n",
        "    print(f\"Token disimpan ke tokens_sample_{i+1}.txt\")\n",
        "\n",
        "# Analisis tambahan: frekuensi kata dari contoh teks\n",
        "all_tokens = []\n",
        "for text in sample_texts:\n",
        "    all_tokens.extend(word_tokenize(text.lower()))\n",
        "\n",
        "word_freq = Counter(all_tokens).most_common(10)\n",
        "print(\"\\n10 Kata Teratas:\", word_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latihan 2: Direct access to a prebuilt corpus via NLTK"
      ],
      "metadata": {
        "id": "GlxTO3anE_Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load sample text from Gutenberg corpus\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')[:1000]  # First 1000 chars of Emma\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Print first 20 tokens\n",
        "print(\"First 20 tokens:\", tokens[:20])\n",
        "\n",
        "# Basic statistics\n",
        "print(\"Total tokens:\", len(tokens))\n",
        "print(\"Unique tokens:\", len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbQeLtxPEbqo",
        "outputId": "9dba54eb-9a91-4d21-e57c-06610c0cf50d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokens: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n",
            "Total tokens: 198\n",
            "Unique tokens: 114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}